\documentclass[a4paper,12pt]{article} % This defines the style of your paper

\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 
\usepackage[utf8]{inputenc} %utf8 % lettere accentate da tastiera
\usepackage[english]{babel} % lingua del documento
\usepackage[T1]{fontenc} % codifica dei font

\usepackage{multirow} % Multirow is for tables with multiple rows within one 
%cell.
\usepackage{booktabs} % For even nicer tables.

\usepackage{graphicx} 

\usepackage{setspace}
\setlength{\parindent}{0in}

\usepackage{float}

\usepackage{fancyhdr}

\usepackage{caption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{color}

\usepackage[hidelinks]{hyperref}
\usepackage{csquotes}
\usepackage{subfigure}

\pagestyle{fancy}

\setlength\parindent{24pt}

\fancyhf{}

\lhead{\footnotesize Deep Learning Lab: Assignment 2}

\rhead{\footnotesize Giorgia Adorni}

\cfoot{\footnotesize \thepage} 

\begin{document}
	

	\thispagestyle{empty}  
	\noindent{
	\begin{tabular}{p{15cm}} 
		{\large \bf Deep Learning Lab} \\
		Università della Svizzera Italiana \\ Faculty of Informatics \\ \today  \\
		\hline
		\\
	\end{tabular} 
	
	\vspace*{0.3cm} 
	
	\begin{center}
		{\Large \bf Assignment 2: Convolutional Neural Network}
		\vspace{2mm}
		
		{\bf Giorgia Adorni (giorgia.adorni@usi.ch)}
		
	\end{center}  
}
	\vspace{0.4cm}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{Introduction}
	The scope of this project is to implement a convolutional neural network to 
	classify the images in the CIFAR-10 dataset.
	
	First of all, the original training set has been shuffled and divided into 
	train and validation sets, with $49000$ and $1000$ images respectively. A 
	seed has been used to reproduce the same sample split and use them in the 
	different models. Instead, the test set provided contains $10000$ images.
	
	A certain preprocessing has been applied to the data. The pixel values of 
	each sample, initially comprised between 0 and 255, have been rescaled 
	between 0 and 1. To represent the class assignments, which were integers 
	between 0 and 9, three binary assignment matrices have been created, one 
	for each set of data. 
	
	The architecture of the convolutional neural network follow the 
	instructions provided, as well as the hyper-parameter values for the models 
	presented in the Sections \ref{section:model0} and \ref{section:dropout}.
	In the training phase, mini-batches were used. In particular, each 
	epoch splits the training set in different samples of data.
	
	All the models were implemented using \texttt{TensorFlow} and trained on a 
	NVIDIA Tesla V100-PCIE-16GB GPU.
	
	\section{Performance of the initial model}
	\label{section:model0}
	In Table \ref{tab:model0} is summarised the architecture of the network 
	used in the first experiment.	
	
	\begin{figure}[H]
		\centering
		
		\begin{tabular}{cccccccc}
		\toprule
		\textbf{conv1} & \textbf{conv2} & \textbf{mpool1} & \textbf{conv3} &
		\textbf{conv4} & \textbf{mpool1} &   \textbf{fc} &
		\textbf{softmax} \\
		\midrule
		3$\times$3,  32 & 3$\times$3, 32 & 2$\times$2 &3$\times$3, 64 & 
		3$\times$3, 64  & 2$\times$2  & 512 & 10\\
		s. 2$\times$2 &   s. 2$\times$2 &   s. 1$\times$1 & s. 1$\times$1  & s. 
		2$\times$2 & s. 2$\times$2 && \\
		p. same & p. same & p. same  & p. same & p. same & p. same &&\\
		\bottomrule
		\end{tabular}
		\captionof{table}{Network architecture}
		\label{tab:model0}
	\end{figure}
	
	The model is trained for $50$ epochs and \texttt{Adam} is used as 
	optimiser with learning rate $0,001$.
	As loss function, the Softmax Cross Entropy with Logits is used since the 
	model is a multi-class classifier. Moreover, once per epoch, is documented 
	the classification accuracy on both the train and validation set.
	The performance are shown in Figure \ref{fig:model0-performance}.
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/1-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/1-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on the initial model}
		\label{fig:model0-performance}
	\end{figure}
	
	As can be seen in Figure (a), the train accuracy rapidly grows up to 
	$1.00$, while the validation accuracy remains stable at $0.70$.  
	The final accuracy on the valid set is $72\%$.
	For what concerns the loss, it is clear that the model overfits the data. 
	In fact, the train loss is next/close to while the validation one diverges.
	
	For this reason, in Section \ref{section:dropout} is presented a new model 
	that has the aim of improving this results.
	
	\section{Regularisation of the model with dropout}
	\label{section:dropout}
	The model proposed in this section involve the use of a model 
	regularisation technique, that is the addition of a dropout layer after 
	each max-pooling and fully-connected layer. In particular, during the 
	training phase, the probability to keep each neuron is setted to $0.5$, 
	while in the validation set should be $1$.	
	
	The architecture of the new network is presented in Table \ref{tab:model1}.	
	
	\begin{figure}[H]
		\centering
		
		\begin{tabular}{cccccccc}
			\toprule
			\textbf{conv1} & \textbf{conv2} & \textbf{mpool1} & 
			\textbf{conv3} &
			\textbf{conv4} & \textbf{mpool2} &   \textbf{fc} &
			\textbf{softmax} \\
			\midrule
			3$\times$3,  32 & 3$\times$3, 32 & 2$\times$2 &3$\times$3, 64 & 
			3$\times$3, 64  & 2$\times$2  & 512 & 10\\
			s. 2$\times$2 &   s. 2$\times$2 &   s. 1$\times$1 & s. 1$\times$1  
			& s. 
			2$\times$2 & s. 2$\times$2 && \\
			p. same & p. same & p. same  & p. same & p. same & p. same &&\\
			 &  & dropout  &  &  & dropout & dropout & dropout\\
			\bottomrule
		\end{tabular}
		\captionof{table}{Network architecture}
		\label{tab:model1}
	\end{figure}
	The actual performance are shown in Figure \ref{fig:model1-performance}.
		
	As can be seen in Figures, the validation performance are legitimately 
	better than the training one, since the regularisation is applied only to 
	the train set and not to the validation.
	
	In this experiment, the validation accuracy is $73.3\%$, a little better 
	respect the previous model.
	Instead, observing the loss curves, with this model there are no signs of 
	overfitting but rather of underfitting. In fact, the training loss is  
	greater than $1.5$ and the validation one is around $1$. Therefore, both 
	have high values. For this reason, in the following section, some 
	experiments will be attempted by modifying the values of the model's 
	hyperparameters.
	
	\begin{figure}[htb]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/2-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/2-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on the regularised model}
		\label{fig:model1-performance}
	\end{figure}
	  
	\section{Hyperparameter settings}
	\label{section:hyperparam}
	
	In this section will be discussed $7$ different configuration for the 
	hyperparameters of the network, with the aim of improving the validation 
	accuracy. In particular, will be documented the performances according to 
	the modification of the following hyperparameters: learning rate, 
	mini-batch size, dropout and number of epochs. In this analysis, only the 
	models with the best performances will be included. In Table 
	\ref{tab:param1} are documented the different hyperparameter settings of 
	these. 
	
	\begin{table}[htb]
		\centering
		\begin{tabular}{l@{\hspace{.5cm}}cccc}
			\toprule
			& \textbf{learning rate} & \textbf{batch size} & \textbf{dropout} & 
			\textbf{epochs} \\
			\midrule
			\textbf{Model 1}  & {1e-3} & {32}  &  -  & 50\\
			\textbf{Model 2}  & {1e-3} & {32}  & 0.5 & 50\\
			\textbf{Model 3}  & {1e-4} & {32}  & 0.5 & 50 \\
			\textbf{Model 4}  & {1e-3} & {128} & 0.6 & 50 \\
			\textbf{Model 5}  & {1e-3} & {128} & 0.5 & 50 \\
			\textbf{Model 6} & {1e-4} & {128} & 0.5 & 50 \\
			\textbf{Model 7}  & {1e-4} & {256} & 0.5 & 50 \\
			\textbf{Model 8}  & {1e-3} & {256} & 0.5 & 50 \\
			\bottomrule 
		\end{tabular}
		\captionof{table}{Model hyperparameters}
		\label{tab:param1}
	\end{table}

	The other configurations tested, also those that include the modification 
	of other hyperparameters, for example using the Gradient Descent as 
	optimiser, will not be presented in this report due to their 
	barely/insufficient results. Other hyperparameters, such as the loss 
	function and the number of hidden units, are never changed. 
	\newline

	The first experiment performed simply consists in the reduction of the 
	learning rate from $0.001$ down to $0.0001$. 
	
	In Figure \ref{fig:model3-performance} are visualised the performances of 
	the model. The accuracy, which now measures $75.5\%$, increased by $2.2\%$ 
	compared to the previous model. Furthermore, it is clearly visible the 
	different trend of the curve. 
	The same can be said for the loss curve which rapidly decreases towards $0$.
	
	In the Subsection \ref{subsection:epochs} will be discussed a further 
	modification to this model that consist in the increasing the number 
	of epochs.

	\begin{figure}[htb]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/3-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/3-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 3}
		\label{fig:model3-performance}
	\end{figure}
	
	In Model 4, the training phase has been carried out using $128$ samples 
	for batch instead of $32$ and the learning rate has been restored to its 
	original value of $0.001$. 
	Furthermore, the dropout value is updated: the probability to keep each 
	neuron is increased to $0.6$. 
	
	In this case, the performance has deteriorated: the accuracy is reduced to 
	$69.7\%$ and the loss is increased, although, as can be seen in Figure 
	\ref*{fig:model5-performance}, the trend of the curve remains promising as 
	in the previous model.
	
	\begin{figure}[htb]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/4-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/4-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 4}
		\label{fig:model4-performance}
	\end{figure}
	
	In Model 5, number or samples for batches and the learning rate are kept 
	equal to the previous experiment, while the dropout has been restored to 
	its original value. 
	The current validation accuracy has begun to rise again, reaching $76.8\%$ 
	and improving the performance of the model 3. However, as it can be seen in 
	Figure \ref{fig:model4-performance}, the curve is growing slower respect to 
	the previous model.
	
	\begin{figure}[htb]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/5-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/5-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 5}
		\label{fig:model5-performance}
	\end{figure}

	As a consequence of the worsening of the performance due to the 
	modification of the dropout, and since the model 3, which has a lower 
	learning rate performs better than the model 2, it was decided to set 
	the learning rate to the previous value of $0.0001$ and restore the dropout 
	to the original value of $0.5$. Instead, the number of examples in the 
	batch for the model 6 has been kept equal to the previous experiments, that 
	is $128$. 
	The curves are shown in Figure \ref{fig:model6-performance}.
		
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/6-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/6-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 6}
		\label{fig:model6-performance}
	\end{figure}

	Unfortunately the results obtained have worsened compared to all the 
	previous performances. This model has a validation accuracy of $64.5\%$, 
	even if the trend of the curve remains promising.
	
	The following model, whose performances are shown in Figure 
	\ref{fig:model7-performance}, foresees an increase in the number of 
	examples per batch up to $256$.
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/7-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/7-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 7}
		\label{fig:model7-performance}
	\end{figure}
	In this case the validation accuracy, which measures $61.8\%$, is the worst 
	measured so far. In fact, compared to the initial model presented this 
	turns out to be even worse $10.8\%$. 
	
	For this reason, it was decided to retrain the model keeping the learning 
	rate value at $0.001$ and the number of examples per batch at $256$.
	As shown in Figure \ref{fig:model8-performance}, the final validation 
	accuracy is $79.4\%$, which is the highest achieved so far.
	
	\begin{figure}[htb]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/8-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/8-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 8}
		\label{fig:model8-performance}
	\end{figure}

	In Table \ref{tab:performace1} are summarised the performance of the 
	presented models. Among the documented model, the best are the model 8, 
	model 5 and model 3. 
	In the following Subsection will be presented the performances of some 
	models after the increase of training epochs.
	
	\begin{table}[H]
		\centering
		\begin{tabular}{l@{\hspace{.5cm}}cc|cc|c}
			\toprule
			& \multicolumn{2}{c}{\textbf{Accuracy}} & 
			\multicolumn{2}{c}{\textbf{Loss}} & \multirow{2}*{\textbf{Train 
					Time}} \\
			& Train & Validation
			& Train & Validation	& 						 		\\
			\midrule
			\textbf{Model 1} & 98.96\% & 72.00\%  & 0.05 & 3.79 & 427 sec \\
			\textbf{Model 2} & 44.46\% & 73.30\%  & 1.48 & 0.89 & 427 sec \\
			\textbf{Model 3} & 46.14\% & \textbf{75.50\%}  & 1.41 & 0.03 & 435 
			sec \\
			\textbf{Model 4} & 36.60\% & 69.70\%  & 1.69 & 1.11 & 148 sec \\
			\textbf{Model 5} & 48.65\% & \textbf{76.80\%}  & 1.32 & 0.74 & 150 
			sec \\
			\textbf{Model 6} & 40.73\% & 64.50\%  & 1.61 & 1.10 & 153 sec \\
			\textbf{Model 7} & 38.72\% & 61.80\%  & 1.29 & 0.03 & 123 sec \\
			\textbf{Model 8} & 48.96\% & \textbf{79.40\%}  & 1.32 & 0.67 & 126 
			sec \\
			\bottomrule 
		\end{tabular}
		\captionof{table}{Model performances}
		\label{tab:performace1}
	\end{table}

	\subsection{Increasing the number of epochs}
	\label{subsection:epochs}
	In this Subsection are analysed the performances of the previous models 
	trained for a greater number of epochs. 
	These experiments will not be carried out only on models that have shown 
	better accuracy values, but also on the others since the learning curves 
	were not yet stabilized. The models are trained on GPU which allows 
	training without worsening the time too much.
		\begin{table}[htb]
		\centering
		\begin{tabular}{l@{\hspace{.5cm}}cccc}
			\toprule
			& \textbf{learning rate} & \textbf{batch size} & \textbf{dropout} & 
			\textbf{epochs} \\
			\midrule
			\textbf{Model 3b} & {1e-4} & {32}  & 0.5 & 300 \\
			\textbf{Model 4b}  & {1e-3} & {128} & 0.6 & 300 \\
			\textbf{Model 5b}  & {1e-3} & {128} & 0.5 & 300 \\
			\textbf{Model 6b} & {1e-4} & {128} & 0.5 & 300 \\
			\textbf{Model 7b}  & {1e-4} & {256} & 0.5 & 300 \\
			\textbf{Model 8b}  & {1e-3} & {256} & 0.5 & 300 \\
			\bottomrule 
		\end{tabular}
		\captionof{table}{New model hyperparameters}
		\label{tab:param}
	\end{table}
	
	The first experiment was performed on model 3.
		
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/3b-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/3b-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 3b}
		\label{fig:model3b-performance}
	\end{figure}

 	In Figure \ref{fig:model3b-performance} are shown the curves. Compared to 
 	the previous, this model has improved the validation accuracy of $9.9\%$, 
	reaching $85.4\%$.
	\newline
	
	The following experiments was carried out on model 4. The results obtained 
	and shown in Figure \ref{fig:model4b-performance} and are worse than those 
	obtained from the previous model.
	
	\begin{figure}[htb]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/4b-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/4b-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 4b}
		\label{fig:model4b-performance}
	\end{figure}
	
	Continuing, the results for the model 5b are shown in Figure 
	\ref{fig:model5b-performance}. Despite the performances of the model on 
	$50$ epochs were among the best, this model obtains lower results compared 
	to model 3b, even if improves the previous ones.
	
	This result was predictable since the learning curve seemed to have already 
	stabilized by training for $50$ epochs.
	
	\begin{figure}[htb]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/5b-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/5b-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 5b}
		\label{fig:model5b-performance}
	\end{figure}
	
	The following model revealed a further increase in performance compared to 
	the previous one and, above all, shows a great difference compared to model 
	trained for $50$ epochs. The current validation accuracy is equal to 
	$82.7\%$. The curves are shown in Figure \ref{fig:model6b-performance}.
	
	\begin{figure}[htb]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/6b-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/6b-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 6b}
		\label{fig:model6b-performance}
	\end{figure}

	The next experiments was carrying out on model 7, the presented the worst 
	performances. In this case, training the model for another $250$ increased 
	the accuracy up to $79.4\%$, increasing it by $17.6\%$. Despite this, the 
	model does not present a further improvement to those previously shown.
	The curves are displayed in Figure \ref{fig:model7b-performance}.
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/7b-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/7b-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 7b}
		\label{fig:model7b-performance}
	\end{figure}

	The last experiment was performed on model 8, which presented the best 
	performances. In this case, the validation accuracy obtained is more or 
	less the same of the one achieved with only $50$ epochs. 
	
	The situation presented, and shown in Figure \ref{fig:model8b-performance}, 
	is very similar to that which occurred with the model 5. This is because 
	after a certain epoch, both the model have stopped learning.
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/8b-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/8b-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 8b}
		\label{fig:model8b-performance}
	\end{figure}

	In Table \ref{tab:performace2} are summarised the performance of the 
	presented models. Among these, the bests are the model 3b, 
	model 6b, model 5b and model 8b. 

	\begin{table}[H]
		\centering
		\begin{tabular}{l@{\hspace{.5cm}}cc|cc|c}
			\toprule
			& \multicolumn{2}{c}{\textbf{Accuracy}} & 
			\multicolumn{2}{c}{\textbf{Loss}} & \multirow{2}*{\textbf{Train 
					Time}} \\
			& Train & Validation
			& Train & Validation	& 						 		\\
			\midrule
			\textbf{Model 3}  & 46.14\% & 75.50\%  & 1.41 & 0.03 & 435 sec \\
			\textbf{Model 3b} & 54.67\% & \textbf{85.40\%}  & 1.08 & 0.46 & 
			2550 sec \\
			\textbf{Model 4}  & 36.60\% & 69.70\%  & 1.69 & 1.11 & 148 sec \\
			\textbf{Model 4b}  & 39.84\% & \textbf{78.40\%}  & 1.58 & 0.86 & 
			916 sec \\
			\textbf{Model 5}  & 48.65\% & 76.80\%  & 1.32 & 0.74 & 150 sec \\
			\textbf{Model 5b} & 53.48\% & \textbf{82.10\%}  & 1.13 & 0.02 & 904 
			sec \\
			\textbf{Model 6}  & 40.73\% & 64.50\%  & 1.61 & 1.10 & 153 sec \\
			\textbf{Model 6b} & 52.23\% & \textbf{82.70\%}  & 1.17 & 0.53 & 904 
			sec \\
			\textbf{Model 7}  & 38.72\% & 61.80\%  & 1.29 & 0.03 & 123 sec \\
			\textbf{Model 7b} & 50.22\% & \textbf{79.40\%}  & 1.25 & 0.66 & 772 
			sec \\
			\textbf{Model 8}  & 48.96\% & 79.40\%  & 1.32 & 0.67 & 126 sec \\
			\textbf{Model 8b} & 54.85\% & \textbf{81.00\%}  & 1.08 & 0.58 & 710 
			sec \\
			\bottomrule 
		\end{tabular}
			\textbf{Model 1} & 98.96\% & 72.00\%  & 0.05 & 3.79 & 427 sec \\
			\textbf{Model 2} & 44.46\% & 73.30\%  & 1.48 & 0.89 & 427 sec \\
			\textbf{Model 2b} & 45.69\% & 73.20\%  & 1.46 & 0.03 & 3006 sec \\
		\captionof{table}{Model performances after increasing the number of 
		epochs}
		\label{tab:performace2}
	\end{table}

			\textbf{Model 4} & 36.60\% & 69.70\%  & 1.69 & 1.11 & 148 sec \\
	The following subsection will present a further attempt to improve 
	performance, or the addition of batch normalisation to the best models.

	\begin{table}[H]
		\centering
		\begin{tabular}{l@{\hspace{.5cm}}cc|cc|c}
			\toprule
			& \multicolumn{2}{c}{\textbf{Accuracy}} & 
			\multicolumn{2}{c}{\textbf{Loss}} & \multirow{2}*{\textbf{Train 
					Time}} \\
			& Train & Validation
			& Train & Validation	& 						 		\\
			\midrule
			\textbf{Model 3} & 46.14\% & 75.50\%  & 1.41 & 0.03 & 435 sec \\
			\textbf{Model 3b} & 54.67\% & \textbf{85.40\%}  & 1.08 & 0.46 & 
			2550 sec \\
			\textbf{Model 3c} & 49.57\% & \textbf{81.60\%}  & 1.29 & 0.55 & 
			4171 sec \\
			\textbf{Model 5} & 48.65\% & 76.80\%  & 1.32 & 0.74 & 150 sec \\
			\textbf{Model 5b} & 48.65\% & 76.80\%  & 1.32 & 0.74 & 150 sec \\
			\textbf{Model 6} & 40.73\% & 64.50\%  & 1.61 & 1.10 & 153 sec \\
			\textbf{Model 6b} & 52.23\% & \textbf{82.70\%}  & 1.17 & 0.53 & 904 
			sec \\
			\textbf{Model 6c} & 47.72\% & \textbf{80.00\%}  & 1.34 & 0.61 & 
			1625 
			sec \\
			\textbf{Model 7} & 38.72\% & 61.80\%  & 1.29 & 0.03 & 123 sec \\
			\textbf{Model 7b} & 50.22\% & 79.40\%  & 1.25 & 0.66 & 772 sec \\
			\textbf{Model 7c} & 50.88\% & 80.30\%  & 1.23 & 0.63 & 772 sec \\
			\bottomrule 
		\end{tabular}
		\captionof{table}{Model performances}
		\label{tab:performace3}
	\end{table}

	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/3c-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/3c-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 3c (with batch 
		normalisation)}
		\label{fig:model3c-performance}
	\end{figure}

	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/6c-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/6c-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 6c (with batch 
		normalisation)}
		\label{fig:model6c-performance}
	\end{figure}

	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/7c-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../src/out/img/7c-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on model 7c (with batch 
		normalisation)}
		\label{fig:model7c-performance}
	\end{figure}
	
	\section{compute the test set accuracy and document the result.} 
	\label{section:finalmodel}
	
	Il training è stato arrestato dopo 300 epoche.
	
%	\begin{figure}[H]
%		\begin{minipage}[c]{.49\textwidth}
%			\centering
%			
%\includegraphics[width=0.85\linewidth]{../src/img/loss/model8-loss-test.jpg}
%			%\caption*{model8-loss-test}
%			%\label{fig:model8-loss-test}
%		\end{minipage}
%		~
%		\begin{minipage}[c]{.49\textwidth}
%			\centering
%			
%\includegraphics[width=1\linewidth]{../src/img/model8-test-polynomial+dataset.png}
%			%\caption*{model8-test-polynomial+dataset}
%			%\label{fig:model8-test-polynomial+dataset}
%		\end{minipage}
%	\caption{Polynomial of 4th degree over $8000$ iterations}
%	\label{fig:model8-test}
%	\end{figure}

\end{document}
