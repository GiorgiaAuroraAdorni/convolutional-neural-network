\documentclass[a4paper,12pt]{article} % This defines the style of your paper

\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 
\usepackage[utf8]{inputenc} %utf8 % lettere accentate da tastiera
\usepackage[english]{babel} % lingua del documento
\usepackage[T1]{fontenc} % codifica dei font

\usepackage{multirow} % Multirow is for tables with multiple rows within one 
%cell.
\usepackage{booktabs} % For even nicer tables.

\usepackage{graphicx} 

\usepackage{setspace}
\setlength{\parindent}{0in}

\usepackage{float}

\usepackage{fancyhdr}

\usepackage{caption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{color}

\usepackage[hidelinks]{hyperref}
\usepackage{csquotes}
\usepackage{subfigure}

\pagestyle{fancy}

\setlength\parindent{24pt}

\fancyhf{}

\lhead{\footnotesize Deep Learning Lab: Assignment 2}

\rhead{\footnotesize Giorgia Adorni}

\cfoot{\footnotesize \thepage} 

\begin{document}
	

	\thispagestyle{empty}  
	\noindent{
	\begin{tabular}{p{15cm}} 
		{\large \bf Deep Learning Lab} \\
		Universit√† della Svizzera Italiana \\ Faculty of Informatics \\ \today  \\
		\hline
		\\
	\end{tabular} 
	
	\vspace*{0.3cm} 
	
	\begin{center}
		{\Large \bf Assignment 2: Convolutional Neural Network}
		\vspace{2mm}
		
		{\bf Giorgia Adorni (giorgia.adorni@usi.ch)}
		
	\end{center}  
}
	\vspace{0.4cm}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{Introduction}
	The scope of this project is to implement a convolutional neural network to 
	classify the images in the CIFAR-10 dataset.
	
	First of all, the original training set has been shuffled and divided into 
	train and validation sets, with $49000$ and $1000$ images respectively. A 
	seed has been used to reproduce the same sample split and use them in the 
	different models. Instead, the test set provided contains $10000$ images.
	
	A certain preprocessing has been applied to the data. The pixel values of 
	each sample, initially comprised between 0 and 255, have been rescaled 
	between 0 and 1. To represent the class assignments, which were integers 
	between 0 and 9, three binary assignment matrices have been created, one 
	for each set of data. 
	
	The architecture of the convolutional neural network follow the 
	instructions provided, as well as the hyper-parameter values for the models 
	presented in the Sections \ref{section:model0} and \ref{section:dropout}.
	In the training phase, mini-batches were used. In particular, each 
	epoch splits the training set in different samples of data.
	
	All the models were implemented using \texttt{TensorFlow} and trained on a 
	NVIDIA Tesla V100-PCIE-16GB GPU.
	
	\section{Performance of the initial model}
	\label{section:model0}
	In Table \ref{tab:model0} is summarised the architecture of the network 
	used in the first experiment.	
	
	\begin{figure}[H]
		\centering
		
		\begin{tabular}{cccccccc}
		\toprule
		\textbf{conv1} & \textbf{conv2} & \textbf{mpool1} & \textbf{conv3} &
		\textbf{conv4} & \textbf{mpool1} &   \textbf{fc} &
		\textbf{softmax} \\
		\midrule
		3$\times$3,  32 & 3$\times$3, 32 & 2$\times$2 &3$\times$3, 64 & 
		3$\times$3, 64  & 2$\times$2  & 512 & 10\\
		s. 2$\times$2 &   s. 2$\times$2 &   s. 1$\times$1 & s. 1$\times$1  & s. 
		2$\times$2 & s. 2$\times$2 && \\
		p. same & p. same & p. same  & p. same & p. same & p. same &&\\
		\bottomrule
		\end{tabular}
		\captionof{table}{Network architecture}
		\label{tab:model0}
	\end{figure}
	
	The model is trained for $50$ epochs and \texttt{Adam} is used as 
	optimiser with learning rate $0,001$.
	As loss function, the Softmax Cross Entropy with Logits is used since the 
	model is a multi-class classifier. Moreover, once per epoch, is documented 
	the classification accuracy on both the train and validation set.
	The performance are shown in Figure \ref{fig:model0-performance}.
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/out/img/1-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/out/img/1-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on the initial model}
		\label{fig:model0-performance}
	\end{figure}
	
	As can be seen in Figure (a), the train accuracy rapidly grows up to 
	$1.00$, while the validation accuracy remains stable at $0.70$.  
	The final accuracy on the valid set is $72\%$.
	For what concerns the loss, it is clear that the model overfits the data. 
	In fact, the train loss is next/close to while the validation one diverges.
	
	For this reason, in Section \ref{section:dropout} is presented a new model 
	that has the aim of improving this results.
	
	\section{Regularisation of the model with dropout}
	\label{section:dropout}
	The model proposed in this section involve the use of a model 
	regularisation technique, that is the addition of a dropout layer after 
	each max-pooling and fully-connected layer. In particular, during the 
	training phase, the probability to keep each neuron is setted to $0.5$, 
	while in the validation set should be $1$.	
	
	The architecture of the new network is presented in Table \ref{tab:model1}.	
	
	\begin{figure}[H]
		\centering
		
		\begin{tabular}{cccccccc}
			\toprule
			\textbf{conv1} & \textbf{conv2} & \textbf{mpool1} & 
			\textbf{conv3} &
			\textbf{conv4} & \textbf{mpool2} &   \textbf{fc} &
			\textbf{softmax} \\
			\midrule
			3$\times$3,  32 & 3$\times$3, 32 & 2$\times$2 &3$\times$3, 64 & 
			3$\times$3, 64  & 2$\times$2  & 512 & 10\\
			s. 2$\times$2 &   s. 2$\times$2 &   s. 1$\times$1 & s. 1$\times$1  
			& s. 
			2$\times$2 & s. 2$\times$2 && \\
			p. same & p. same & p. same  & p. same & p. same & p. same &&\\
			 &  & dropout  &  &  & dropout & dropout & dropout\\
			\bottomrule
		\end{tabular}
		\captionof{table}{Network architecture}
		\label{tab:model1}
	\end{figure}
	The performance are shown in Figure \ref{fig:model1-performance}.
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/out/img/2-Accuracy.png}
			\caption*{(a)}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/out/img/2-Loss.png}
			\caption*{(b)}
		\end{minipage}
		\caption{Training and validation loss on the regularised model}
		\label{fig:model1-performance}
	\end{figure}
	  
	
	
	\section{hyperparameter settings}
	\label{section:hyperparam}

	\section{compute the test set accuracy and document the result.} 
	\label{section:finalmodel}

\end{document}
